{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LJlpNsyqCqW"
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSHOkKD_p0Jh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMcW4tFcqTBB"
   },
   "source": [
    "# Processing the data.tsv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkycVexF7YqV"
   },
   "source": [
    "## For proper operation, copy the file \"data.tsv\" into the colab directory. To navigate there, just press the \"files\" button on the left of the window, then press Right click--> upload, then choose the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvDiU6Ciquvb"
   },
   "source": [
    "## Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgA3ayc-p6Cn"
   },
   "outputs": [],
   "source": [
    "tsv_file = open(\"data.tsv\")\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "\n",
    "# convert read data into an array\n",
    "text_list = []\n",
    "for i, row in enumerate(read_tsv):\n",
    "    if(i == 0):\n",
    "        continue\n",
    "    else:\n",
    "        text_list.append(row)\n",
    "tsv_file. close()\n",
    "\n",
    "text_array = np.array(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XdefhcvqyO3"
   },
   "source": [
    "## Checking that there are equal number of subjective and objective examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pom2zuh47YqW"
   },
   "source": [
    "### \"check_equal_subjective_objective\" function\n",
    "\n",
    "DESCRIPTION: checks whether the dataset has an equal number of subjective (represented with label '1')\n",
    "      and objective (represented with label '0') examples\n",
    "\n",
    "INPUT:\n",
    "\n",
    "    input_array: array for the dataset\n",
    "    \n",
    "    dataset_name: name associated with the dataset\n",
    "    \n",
    "OUTPUT:\n",
    "\n",
    "    subjective_counter: number of subjective examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGkFPsV2qqSj"
   },
   "outputs": [],
   "source": [
    "def check_equal_subjective_objective(input_array, dataset_name):\n",
    "    subjective_counter = 0\n",
    "    objective_counter = 0\n",
    "    for i in range(input_array.shape[0]):\n",
    "        if (input_array[i, 1] == '1'):\n",
    "            subjective_counter += 1\n",
    "        else:\n",
    "            objective_counter += 1\n",
    "\n",
    "    if (subjective_counter == objective_counter):\n",
    "        print(\"equal number of subjective & objective examples in the \" + dataset_name + \" dataset\")\n",
    "    else:\n",
    "        print(\"unequal number of subjective & objective examples in the \" + dataset_name + \" dataset\")\n",
    "        exit()\n",
    "\n",
    "    return subjective_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuNMCHZ47YqX"
   },
   "source": [
    "### Execute the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91bai0-wq-TY",
    "outputId": "77ff8ec3-a9dc-4987-b99c-5efbe39741f7"
   },
   "outputs": [],
   "source": [
    "subjective_counter = check_equal_subjective_objective(text_array, \"whole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRZ7dK3j1El7"
   },
   "source": [
    "## Split data into train, validation & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sllXFxhG7YqY"
   },
   "source": [
    "### \"split_subjective_objective\" function\n",
    "\n",
    "  DESCRIPTION: Given the input dataset and the number of subjective examples, split\n",
    "      the dataset into 2 arrays, one has only subjective examples, and the\n",
    "      other has only objective examples\n",
    "\n",
    "  INPUT:\n",
    "  \n",
    "    text_array: input dataset to be split\n",
    "\n",
    "    num_subjective_examples: number of subjective examples\n",
    "\n",
    "  OUTPUT:\n",
    "    \n",
    "    subjective_array: array with subjective-only examples\n",
    "    \n",
    "    objective_array: array with objective-only examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G0Z83r71weo"
   },
   "outputs": [],
   "source": [
    "def split_subjective_objective(text_array, num_subjective_examples):\n",
    "    # create empty arrays for each label type\n",
    "    # second dimension with shape 2: 1st dimension is input, 2nd is label\n",
    "    subjective_array = np.empty((num_subjective_examples, 2), dtype=object)\n",
    "    objective_array = np.empty((num_subjective_examples, 2), dtype=object)\n",
    "\n",
    "    subjective_counter = 0\n",
    "    objective_counter = 0\n",
    "    for i in range(text_array.shape[0]):\n",
    "        # label '1' is subjective, label '0' is objective\n",
    "        if (text_array[i, 1] == '1'):\n",
    "            subjective_array[subjective_counter, :] = text_array[i, :]\n",
    "            subjective_counter += 1\n",
    "        else:\n",
    "            objective_array[objective_counter, :] = text_array[i, :]\n",
    "            objective_counter += 1\n",
    "\n",
    "    return subjective_array, objective_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le2gUm0p7YqY"
   },
   "source": [
    "### \"create_dataset\" function\n",
    "\n",
    "DESCRIPTION:\n",
    "    given subjective and objective arrays, extract subarrays from each and\n",
    "    combine these subarrays into one array. The subarrays are extracted according\n",
    "    to their start and end indices\n",
    "\n",
    "INPUT:\n",
    "\n",
    "    subjective_array: array with subjective-only examples\n",
    "    \n",
    "    objective_array: array with objective-only examples\n",
    "    \n",
    "    start_index: start index for the subarrays\n",
    "    \n",
    "    end_index: end index for the subarrays\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "    dataset: array which consists of the subarrays from the subjetive and\n",
    "    \n",
    "    objective arrays. This output array is shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5lmsdWZ7YqY"
   },
   "outputs": [],
   "source": [
    "def create_dataset(subjective_array, objective_array, start_index, end_index):\n",
    "  subjective_set = subjective_array[start_index : end_index, ...]\n",
    "  objective_set = objective_array[start_index : end_index, ...]\n",
    "  dataset = np.concatenate((subjective_set, objective_set), axis=0)\n",
    "  # shuffling the dataset\n",
    "  np.random.seed(0)\n",
    "  np.random.shuffle(dataset)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSer1Fqc7YqY"
   },
   "source": [
    "### \"check_uniqueness_sample\" function\n",
    "\n",
    "\n",
    "DESCRIPTION:\n",
    "      Given the whole dataset and each of the training, validation, and test\n",
    "      sets, check that each example in the whole dataset is found in only\n",
    "      one of the 3 sets (unique to that set).\n",
    "      \n",
    "INPUT:\n",
    "\n",
    "      text_array: whole dataset\n",
    "      \n",
    "      training_set: training dataset\n",
    "      \n",
    "      validation_set: validation dataset\n",
    "      \n",
    "      test_set: test dataset\n",
    "\n",
    "OUTPUT:\n",
    "      \n",
    "      None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw0IA-pZ7YqZ"
   },
   "outputs": [],
   "source": [
    "def check_uniqueness_sample(text_array, training_set, validation_set, test_set):\n",
    "\n",
    "    # convert array of input examples to list\n",
    "    training_list = training_set[:, 0].tolist()\n",
    "    validation_list = validation_set[:, 0].tolist()\n",
    "    test_list = test_set[:, 0].tolist()\n",
    "\n",
    "    # counter for the number of occurences of an example in the 3 sets\n",
    "    detection_counter = 0\n",
    "\n",
    "    # in case an example is found in more than one set, check the rest of the\n",
    "    # examples before terminating the code\n",
    "    exit_later = False\n",
    "\n",
    "    for i in range(text_array.shape[0]):\n",
    "        if (text_array[i, 0] in training_list):\n",
    "            detection_counter += 1\n",
    "        if (text_array[i, 0] in validation_list):\n",
    "            detection_counter += 1\n",
    "        if (text_array[i, 0] in test_list):\n",
    "            detection_counter += 1\n",
    "\n",
    "        # detect whether an example is duplicated\n",
    "        if (detection_counter > 1):\n",
    "            print(f\"example {i} is repeated in more than one dataset\")\n",
    "            exit_later = True\n",
    "        # detect whether an example is not included in any of the 3 sets\n",
    "        elif (detection_counter == 0):\n",
    "            print(f\"example {i} is not found in any dataset\")\n",
    "            exit_later = True\n",
    "\n",
    "        detection_counter = 0\n",
    "\n",
    "    if (exit_later):\n",
    "        exit()\n",
    "\n",
    "    print(\"training, validation, and test datasets all include unique examples\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWBlKel57YqZ"
   },
   "source": [
    "#### Split the dataset into the training, validation, and test sets by executing the 4 functions \"split_subjective_objective\", \"create_dataset\", \"check_equal_subjective_objective\", \"check_uniqueness_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CedoIwUE7YqZ"
   },
   "source": [
    "##### split data into subjective and objective arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiklXEZ67YqZ"
   },
   "outputs": [],
   "source": [
    "num_subjective_examples = subjective_counter\n",
    "subjective_array, objective_array = split_subjective_objective(text_array, num_subjective_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zyvWNJ27YqZ"
   },
   "source": [
    "##### calculate the size of the training and validation sets (this is to get the start and end indices of their arrays for splitting the whole array ahead). Start and end indices can be deduced from these calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k5XrlAy7YqZ"
   },
   "outputs": [],
   "source": [
    "# shuffle each array so that they don't have the same order as the data.tsv file\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(subjective_array)\n",
    "np.random.shuffle(objective_array)\n",
    "\n",
    "# training set is 64% of the total, validation is 16% and test set is the\n",
    "# remaining 20%\n",
    "train_examples_size = int(0.64 * num_subjective_examples) # size of training set\n",
    "validation_examples_size = int(0.16 * num_subjective_examples)  # size of validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r85fgT4I7YqZ"
   },
   "source": [
    "##### split the whole dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQthkb8t1LHN"
   },
   "outputs": [],
   "source": [
    "# to make sure there is no overlap, the end index for one set must be the start\n",
    "# index for the next set\n",
    "train_set = create_dataset(subjective_array, objective_array, 0, train_examples_size)\n",
    "validation_set = create_dataset(subjective_array, objective_array, train_examples_size,\n",
    "                                (train_examples_size + validation_examples_size))\n",
    "test_set = create_dataset(subjective_array, objective_array, (train_examples_size + validation_examples_size),\n",
    "                          num_subjective_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5goh6qj7Yqa"
   },
   "source": [
    "##### verify that there are equal number of subjective and objective examples in each dataset and then check that every example is unique to only one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvtkbPPK7Yqa",
    "outputId": "c74c2153-d608-470c-a12d-2fb3c869a375"
   },
   "outputs": [],
   "source": [
    "# verification checks\n",
    "check_equal_subjective_objective(train_set, \"training set\")\n",
    "check_equal_subjective_objective(validation_set, \"validation set\")\n",
    "check_equal_subjective_objective(test_set, \"test set\")\n",
    "check_uniqueness_sample(text_array, train_set, validation_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xom_jveK_WL6"
   },
   "source": [
    "## saving each dataset into a .tsv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F4YiMTd7Yqa"
   },
   "source": [
    "### convert each array to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oS7IdnJ7Yqa"
   },
   "outputs": [],
   "source": [
    "# convert arrays to pandas dataframes\n",
    "dataframe_col_names = [\"text\", \"label\"]\n",
    "train_dataframe = pd.DataFrame(train_set, columns = dataframe_col_names)\n",
    "validation_dataframe = pd.DataFrame(validation_set, columns = dataframe_col_names)\n",
    "test_dataframe = pd.DataFrame(test_set, columns = dataframe_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpaukTyZ7Yqa"
   },
   "source": [
    "### save each pandas dataframe as a .tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELJXAQQv_ahX"
   },
   "outputs": [],
   "source": [
    "# store the datasets in .tsv files\n",
    "# path should be \"./data/{dataset_name}.tsv\"\n",
    "train_dataframe.to_csv(\"train.tsv\", index = None, sep=\"\\t\")\n",
    "validation_dataframe.to_csv(\"validation.tsv\", index = None, sep=\"\\t\")\n",
    "test_dataframe.to_csv(\"test.tsv\", index = None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG_QKlKP_w6N"
   },
   "source": [
    "## Final text processing and converting examples into torch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPMPVvHl7Yqa"
   },
   "source": [
    "### \"TextDataset\" class\n",
    "\n",
    "#### the following class converts the input dataset torch tensors convert every word in the inputs to its corresponding token according to the used embedding matrix (GloVe in our case), & any word that is not represented in the embedding matrix gets an \"out-of-vocabulary\" token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LfjG7IJSADr"
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, split=\"train\"):\n",
    "        data_path = \"data\"\n",
    "        df = pd.read_csv(f\"{split}.tsv\", sep=\"\\t\")\n",
    "\n",
    "        # X: torch.tensor (maxlen, batch_size), padded indices\n",
    "        # Y: torch.tensor of len N\n",
    "        X, Y = [], []\n",
    "        V = len(vocab.vectors)\n",
    "        for i, row in df.iterrows():\n",
    "            # split input sentence into a list of its words\n",
    "            L = row[\"text\"].split()\n",
    "            # convert each word in the sentence with its corresponding token\n",
    "            # and convert the output vector into a torch tensor\n",
    "            X.append(torch.tensor([vocab.stoi.get(w, V - 1) for w in L]))  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
    "            Y.append(row.label)\n",
    "        self.X = X\n",
    "        self.Y = torch.tensor(Y)    # convert labels into a torch tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYpWCp5m7Yqa"
   },
   "source": [
    "### \"my_collate_function\" function\n",
    "\n",
    "DESCRIPTION: this function prepares batches for being used with the model. batch is approximately: [dataset[i] for i in range(0, batch_size)]. Since the dataset[i]'s contents is defined in the __getitem__() above, this collate function should be set correspondingly. Also: collate_function just takes one argument. To pass in additional arguments (e.g., device), we need to wrap up an anonymous function (using lambda, as will be shown when executing this function). Padding for the text is handled here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0thHkahv7Yqa"
   },
   "outputs": [],
   "source": [
    "def my_collate_function(batch, device):\n",
    "    batch_x, batch_y = [], []\n",
    "    max_len = 0\n",
    "    for x, y in batch:\n",
    "        batch_y.append(y)\n",
    "        max_len = max(max_len, len(x))\n",
    "    for x, y in batch:\n",
    "        x_p = torch.concat(\n",
    "            [x, torch.zeros(max_len - len(x))]\n",
    "        )\n",
    "        batch_x.append(x_p)\n",
    "    return torch.stack(batch_x).t().int().to(device), torch.tensor(batch_y).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYRmJ2siA1oO"
   },
   "source": [
    "# Create and train the MLP (Single Neuron) classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iiZ5Y9ZA_vY"
   },
   "source": [
    "## \"Classifier model\" class\n",
    "\n",
    "### this class describes the architecture of the classifier model used in this notebook. We declare the layers used in the model in the __init__ function, then we outline how these layers are organized/stacked in the forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL8MAOkhBCZI"
   },
   "outputs": [],
   "source": [
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_vectors):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = vocab_vectors.shape[1]\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(vocab_vectors) # embedding layer\n",
    "        self.linear_NN = torch.nn.Linear(self.embedding_size, 1) # fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.tensor of shape (bsz, sentence_max_len)\n",
    "        \"\"\"\n",
    "        # TO DO\n",
    "        bsz = int(x.shape[0])\n",
    "        max_len = int(x.shape[1])\n",
    "\n",
    "        input_embedding_matrix = self.embedding(x) # shape : (max_len, embedding_size)\n",
    "        averaged_embedding = torch.mean(input_embedding_matrix, dim = 1) # shape : (embedding_size)\n",
    "        linear_output = self.linear_NN(averaged_embedding)\n",
    "\n",
    "        return linear_output, input_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc82EBRWBXfi"
   },
   "source": [
    "## Functions that are used for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXCCwDcU7Yql"
   },
   "source": [
    "### \"prediction_accuracy\" function\n",
    "\n",
    "DESCRIPTION:\n",
    "      this function takes the linear output of the NN and applies the sigmoid\n",
    "      and the decision function, then it computes the accuracy\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "      predicted: predicted values from the model for a certain batch\n",
    "      \n",
    "      ground_truth: ground-truth values for each batch\n",
    "      \n",
    "      sigmoid_fn: sigmoid activation function\n",
    "      \n",
    "OUTPUTS:\n",
    "\n",
    "      average_accuracy_per_batch: average accuracy per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_zuNpdn7Yql"
   },
   "outputs": [],
   "source": [
    "# the prediction_accuracy function is defined first as it is used in the fit function\n",
    "def prediction_accuracy(predicted, ground_truth, sigmoid_fn):\n",
    "    bsz = ground_truth.shape[0]   # batch size\n",
    "\n",
    "    sigmoid_output = sigmoid_fn(predicted)\n",
    "\n",
    "    # count the number of correct predictions\n",
    "    accuracy_counter = 0\n",
    "    for i in range(bsz):\n",
    "        if (sigmoid_output[i] == 0.5):    # if it's neither subjective or objective, then it's still a misclassification\n",
    "            continue\n",
    "        elif (sigmoid_output[i] > 0.5 and ground_truth[i] == 1):\n",
    "            accuracy_counter += 1\n",
    "        elif (sigmoid_output[i] < 0.5 and ground_truth[i] == 0):\n",
    "            accuracy_counter += 1\n",
    "\n",
    "    average_accuracy_per_batch = accuracy_counter / bsz\n",
    "\n",
    "    return average_accuracy_per_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW7kdF_57Yql"
   },
   "source": [
    "### \"Batch_iteration\" function\n",
    "\n",
    "DESCRIPTION: get outputs of the model for a single batch of training/validation/test data. Also, the function calculates the batch loss and accuracy and adds that to the loss and accuracy of the whole epoch\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "    batch: batch that is input to the model\n",
    "\n",
    "    batch_size: size of the batch used\n",
    "\n",
    "    optimizer: optimzer used for training\n",
    "    \n",
    "    loss_fn: loss function used for training\n",
    "    \n",
    "    running_loss: running loss for the epoch\n",
    "    \n",
    "    running_accuracy: running accuracy for the epoch\n",
    "    \n",
    "    NN_model: classifier model\n",
    "    \n",
    "    training_batch = False: flag to determine whether this is a training batch or validation/test batch\n",
    "\n",
    "OUTPUTS:\n",
    "\n",
    "    running_loss: running loss for the epoch after adding the batch's loss\n",
    "    \n",
    "    running_accuracy: running accuracy for the epoch after adding the batch's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZrEm7qI7Yql"
   },
   "outputs": [],
   "source": [
    "def Batch_iteration(batch, batch_size, running_loss, running_accuracy, NN_model,\n",
    "                    prediction_sigmoid_fn, training_batch = False, optimizer = None, loss_fn = None):\n",
    "    inputs = batch[0]   # input examples of the batch\n",
    "    labels = batch[1]   # ground-truth values for each input in the batch\n",
    "\n",
    "    # check whether the \"inputs\" vector should be transposed to be used correctly\n",
    "    if (inputs.shape[0] != batch_size):\n",
    "        assert inputs.shape[1] == batch_size, \"shape of input is inconsistent with batch size\"\n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "\n",
    "    if training_batch:\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Make predictions for this batch\n",
    "    predicted_outputs, input_embeddings = NN_model.forward(inputs)\n",
    "\n",
    "    # make sure that labels and predicted_outputs will have matching types and shapes\n",
    "    if (len(predicted_outputs.shape) > 1):\n",
    "        predicted_outputs = torch.squeeze(predicted_outputs)\n",
    "    if (labels.dtype != torch.float32):\n",
    "        labels = labels.float()\n",
    "\n",
    "    # Compute the loss\n",
    "    if loss_fn is not None:\n",
    "      loss = loss_fn(predicted_outputs, labels)\n",
    "    else:\n",
    "      loss = 0\n",
    "\n",
    "    # compute gradients for the loss\n",
    "    if training_batch:\n",
    "        loss.backward()\n",
    "\n",
    "    running_loss += loss\n",
    "\n",
    "    # get training accuracy\n",
    "    accuracy = prediction_accuracy(predicted_outputs, labels, prediction_sigmoid_fn)\n",
    "    running_accuracy += accuracy\n",
    "\n",
    "    if training_batch:\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return running_loss, running_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgN5u4De_xxH"
   },
   "source": [
    "### \"plot_2curves\" function\n",
    "\n",
    "DESCRIPTION: plot 2 curves on the same figure\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "    x: array of inputs for each curve (should be the same for the 2 curves)\n",
    "  \n",
    "    y1: array of outputs for the first curve\n",
    "  \n",
    "    y2: array of outputs for the second curve\n",
    "  \n",
    "    y1_label: label for the first curve\n",
    "  \n",
    "    y2_label: label for the second curve\n",
    "  \n",
    "    plot_title: title of the figure\n",
    "\n",
    "OUTPUTS:\n",
    "\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpNkyCwF_ygx"
   },
   "outputs": [],
   "source": [
    "def plot_2curves(x, y1, y2, y1_label, y2_label, plot_title):\n",
    "  plt.figure()\n",
    "  plt.plot(x, y1, color=\"b\", label=y1_label)\n",
    "  plt.plot(x, y2, color=\"r\", label=y2_label)\n",
    "  plt.title(plot_title)\n",
    "  plt.legend()\n",
    "  plt.show(block = False)\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEwQpICb7Yql"
   },
   "source": [
    "### \"model_fit\" function\n",
    "\n",
    "DESCRPTION:\n",
    "      fit the given model to the given training set, and check whether it's\n",
    "      generalizing well to the validation set. After training is completed, test\n",
    "      the final performance of the model on the test set\n",
    "\n",
    "INPUT:\n",
    "\n",
    "      NN_model: model that will be trained\n",
    "\n",
    "      train_dataloader: object that stores batches of the training set\n",
    "\n",
    "      validation_dataloader: object that stores batches of the validation set\n",
    "\n",
    "      test_dataloader: object that stores batches of the test set\n",
    "\n",
    "      batch_size = 4: batch size used (default value is 4)\n",
    "\n",
    "      learning_rate = 0.001: learning rate used for optimization (default value is 0.001)\n",
    "\n",
    "      EPOCHS = 50: number of passes through the whole training set (default value is 50)\n",
    "\n",
    "      l2 = 0: L2 regularization parameter (default value is 0)\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "      None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHwUehcrBhBe"
   },
   "outputs": [],
   "source": [
    "def model_fit(NN_model, train_dataloader,validation_dataloader, test_dataloader,\n",
    "                  batch_size = 4, learning_rate = 0.001, EPOCHS = 50, l2 = 0):\n",
    "\n",
    "    np.random.seed(2)\n",
    "    torch.manual_seed(2)\n",
    "\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(NN_model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "\n",
    "    # number of batches for each dataset\n",
    "    training_batches = len(train_dataloader)\n",
    "    validation_batches = len(validation_dataloader)\n",
    "    test_batches = len(test_dataloader)\n",
    "\n",
    "    # arrays for tracking the loss values for training and validation sets for every epoch\n",
    "    training_loss_per_epoch = np.zeros(EPOCHS)\n",
    "    validation_loss_per_epoch = np.zeros(EPOCHS)\n",
    "\n",
    "    # arrays for tracking the accuracies for training and validation sets for every epoch\n",
    "    training_accuracy_per_epoch = np.zeros(EPOCHS)\n",
    "    validation_accuracy_per_epoch = np.zeros(EPOCHS)\n",
    "\n",
    "    prediction_sigmoid_fn = torch.nn.Sigmoid() # used for getting prediction accuracy\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_train_loss = 0.0\n",
    "        running_train_accuracy = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            running_train_loss, running_train_accuracy = \\\n",
    "            Batch_iteration(batch, batch_size, running_train_loss, running_train_accuracy, NN_model, \\\n",
    "                            prediction_sigmoid_fn, training_batch = True, optimizer=optimizer, loss_fn=loss_fn)\n",
    "\n",
    "        # average training loss for this epoch\n",
    "        avg_train_loss = running_train_loss / training_batches\n",
    "        training_loss_per_epoch[epoch] = avg_train_loss\n",
    "\n",
    "        # average training accuracy for this epoch\n",
    "        avg_train_accuracy = running_train_accuracy / training_batches\n",
    "        training_accuracy_per_epoch[epoch] = avg_train_accuracy\n",
    "\n",
    "        running_validation_loss = 0.0\n",
    "        running_validation_accuracy = 0.0\n",
    "\n",
    "        # validation loop\n",
    "        for batch in validation_dataloader:\n",
    "          running_validation_loss, running_validation_accuracy = \\\n",
    "          Batch_iteration(batch, batch_size, running_validation_loss, running_validation_accuracy, \\\n",
    "                          NN_model, prediction_sigmoid_fn, training_batch = False, optimizer=optimizer, loss_fn=loss_fn)\n",
    "\n",
    "        # average validation loss for this epoch\n",
    "        avg_validation_loss = running_validation_loss / validation_batches\n",
    "        validation_loss_per_epoch[epoch] = avg_validation_loss\n",
    "\n",
    "        # average validation accuracy for this epoch\n",
    "        avg_validation_accuracy = running_validation_accuracy / validation_batches\n",
    "        validation_accuracy_per_epoch[epoch] = avg_validation_accuracy\n",
    "\n",
    "        print(f'EPOCH: {epoch} \\t LOSS train {avg_train_loss:.3f}, \\t valid {avg_validation_loss:.3f}, '\n",
    "              f'ACC train {avg_train_accuracy:.3f}, \\t valid {avg_validation_accuracy:.3f}')\n",
    "\n",
    "    # plot training and validation losses across the epochs\n",
    "    plot_2curves(np.arange(EPOCHS), training_loss_per_epoch, \\\n",
    "                 validation_loss_per_epoch, \"train loss\", \"validation loss\", \"Training & validation losses\")\n",
    "\n",
    "    # plot training and validation accuracies across the epochs\n",
    "    plot_2curves(np.arange(EPOCHS), training_accuracy_per_epoch, \\\n",
    "                 validation_accuracy_per_epoch, \"train accuracy\", \"validation accuracy\", \"Training & validation accuracies\")\n",
    "\n",
    "    # plot training loss and accuracy across the epochs\n",
    "    plot_2curves(np.arange(EPOCHS), training_loss_per_epoch, \\\n",
    "                 training_accuracy_per_epoch, \"train loss\", \"train accuracy\", \"Training loss & accuracy\")\n",
    "\n",
    "    # test loop\n",
    "    running_test_accuracy = 0.0\n",
    "    running_test_loss = 0.0   # we don't actually care about this loss, we're just declaring it for the  Batch_iteration function\n",
    "    for batch in test_dataloader:\n",
    "      _, running_test_accuracy = \\\n",
    "      Batch_iteration(batch, batch_size, running_test_loss, running_test_accuracy, NN_model, \\\n",
    "                            prediction_sigmoid_fn, training_batch = False)\n",
    "\n",
    "    avg_test_accuracy = running_test_accuracy / test_batches\n",
    "\n",
    "    print(f'Test accuracy is {avg_test_accuracy:.3f}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXh5VnrP80ys"
   },
   "source": [
    "## \"model_test\" function\n",
    "\n",
    "DESCRIPTION:\n",
    "      test the trained model on the test set\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "      NN_model: classifier model\n",
    "\n",
    "      test_dataloader: objects that stores batches of the test set\n",
    "\n",
    "      batch_size = 4: size of the batch that's fed to the model (default value is 4)\n",
    "\n",
    "OUTPUTS:\n",
    "\n",
    "      None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXkUd1L5889F"
   },
   "outputs": [],
   "source": [
    "def model_test(NN_model, test_dataloader, batch_size = 4):\n",
    "    np.random.seed(2)\n",
    "    torch.manual_seed(2)\n",
    "\n",
    "    test_batches = len(test_dataloader)\n",
    "\n",
    "    prediction_sigmoid_fn = torch.nn.Sigmoid() # used for getting prediction accuracy\n",
    "\n",
    "    # test loop\n",
    "    running_test_accuracy = 0.0\n",
    "    running_test_loss = 0.0 # we don't actually care about this loss, we're just declaring it for the  Batch_iteration function\n",
    "    for batch in test_dataloader:\n",
    "        _, running_test_accuracy = \\\n",
    "      Batch_iteration(batch, batch_size, running_test_loss, running_test_accuracy, NN_model, \\\n",
    "                            prediction_sigmoid_fn)\n",
    "\n",
    "    avg_test_accuracy = running_test_accuracy / test_batches\n",
    "\n",
    "    print(f'Test accuracy is {avg_test_accuracy:.3f}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-ajWCxzRE_T"
   },
   "source": [
    "# Actual training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EZtaXbuB8zN"
   },
   "source": [
    "## load the glove embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njndA5iSv5dO",
    "outputId": "8e20ab24-b21c-4050-8331-56386cdf3d76"
   },
   "outputs": [],
   "source": [
    "# load the glove embedding matrix for use in the model\n",
    "\n",
    "# The first time you run this will download a 862MB size file to .vector_cache/glove.6B.zip\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=100)  # embedding size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpEcmlODCIZ3"
   },
   "source": [
    "## set the hyperparameters, and choose the processor used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "At4WWTaOCJAd",
    "outputId": "319e6493-1823-49b9-a261-9d4bed2d93df"
   },
   "outputs": [],
   "source": [
    "# hyperparameters for model training\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "l2 = 0\n",
    "EPOCHS=10\n",
    "\n",
    "#   fix seed\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# use GPU if possible, if not, then CPU is automatically used\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-mkmu5BCUf6"
   },
   "source": [
    "## define the dataloader for the training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27h6RhyPCa3-"
   },
   "outputs": [],
   "source": [
    "# convert each dataset into its equivalent torch tensor\n",
    "train_dataset = TextDataset(glove, \"train\")\n",
    "val_dataset = TextDataset(glove, \"validation\")\n",
    "test_dataset = TextDataset(glove, \"test\")\n",
    "\n",
    "# objects that handle batching of torch tensors\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: my_collate_function(batch, device), drop_last = True)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: my_collate_function(batch, device), drop_last = True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: my_collate_function(batch, device), drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97Xq_alOCj-X"
   },
   "source": [
    "## instantiate the model and execute the fit function. Save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "43RqC9c-V5Qm",
    "outputId": "15064f59-463d-438f-ff55-40f328eb888b"
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "Classifier_NN = ClassifierModel(glove.vectors)\n",
    "\n",
    "start_time = time.time()\n",
    "# call the model fit function to train it\n",
    "model_fit(Classifier_NN, train_dataloader, validation_dataloader, test_dataloader,\n",
    "              batch_size= batch_size, learning_rate= learning_rate, EPOCHS= EPOCHS,l2= l2)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"training time: {training_time/60 :.2f} min\")\n",
    "\n",
    "# save model parameters\n",
    "torch.save(Classifier_NN.state_dict(), 'classifier_model.pt') # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvVGdSrDq--U"
   },
   "source": [
    "# Interesting experiment\n",
    "\n",
    "Since the model has a single neuron with its weight vector having the same size of the embedding vectors, why don't we assume that this weight vector is itself an embedding and check which words are closest to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8ga0Pg6ddB3"
   },
   "outputs": [],
   "source": [
    "def print_closest_cosine_words(glove, vec, n=5):\n",
    "    dists = torch.cosine_similarity(glove.vectors, vec.unsqueeze(0), dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1], reverse=True) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]:                         # take the top n\n",
    "         print(glove.itos[idx], \"\\t%5.2f\" % difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABkSf1rl1bhE",
    "outputId": "2ebcf2af-0084-4137-df37-9dc4d22651be"
   },
   "outputs": [],
   "source": [
    "final_layer_weights = Classifier_NN.linear_NN.weight.data\n",
    "final_layer_weights = torch.squeeze(final_layer_weights)\n",
    "\n",
    "print_closest_cosine_words(glove, final_layer_weights, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r100-WCBXn3S"
   },
   "source": [
    "# Gradio App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFQaEwCfDWiQ"
   },
   "source": [
    "## install gradio and import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5UOXgTDt-sC",
    "outputId": "5013828f-c4d8-4dda-aa79-0ab4f860a024"
   },
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4efzF1aDUTd"
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDLXQ9wPuiKD"
   },
   "source": [
    "## Sentence classification functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x3SvINsDj3X"
   },
   "source": [
    "### \"tokenize_sentence\" function\n",
    "\n",
    "DESCRIPTION:\n",
    "      convert input sentence into its corresponding tokens vector\n",
    "      \n",
    "INPUTS:\n",
    "\n",
    "      input_sentence: sentence to be tokenized\n",
    "\n",
    "OUTPUTS:\n",
    "\n",
    "      token_tensor: tokenized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV6vsaK9DhYx"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(input_sentence):\n",
    "    tokens = input_sentence.split()\n",
    "    # Convert to integer representation per token\n",
    "    token_ints = [glove.stoi.get(tok, len(glove.stoi) - 1) for tok in tokens]\n",
    "    # Convert into a tensor of the shape accepted by the models\n",
    "    token_tensor = torch.LongTensor(token_ints).view(-1, 1)\n",
    "\n",
    "    return token_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJwpOGa4DzvP"
   },
   "source": [
    "### \"classify_sentence\" function\n",
    "\n",
    "  DESCRIPTION:\n",
    "      using the given model and tokenized sentence, classify whether this\n",
    "      sentence is subjective or objective\n",
    "\n",
    "  INPUTS:\n",
    "\n",
    "      NN_model: model used for classification\n",
    "\n",
    "      tokenized_sentence: sentence to be classified after being tokenized\n",
    "\n",
    "  OUTPUTS:\n",
    "\n",
    "      classification: classification, whether subjective or objective\n",
    "\n",
    "      subjectivity_probability: probability/confidence that model thinks the sentence is subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEy02dM5mp14"
   },
   "outputs": [],
   "source": [
    "def classify_sentence(NN_model, tokenized_sentence):\n",
    "    # make sure that batch size is 1\n",
    "    if (tokenized_sentence.shape[0] != 1):\n",
    "        assert tokenized_sentence.shape[1] == 1\n",
    "        tokenized_sentence_updated = torch.transpose(tokenized_sentence, 0, 1)\n",
    "    else:\n",
    "        tokenized_sentence_updated = tokenized_sentence\n",
    "\n",
    "    linear_output, *_ = NN_model(tokenized_sentence_updated)\n",
    "    sigmoid_fn = torch.nn.Sigmoid()\n",
    "    sigmoid_output = sigmoid_fn(linear_output)\n",
    "\n",
    "    # We assume here that if the output of the sigmoid is 0.5, then it's classified as 0 (objective)\n",
    "    if (sigmoid_output > 0.5):\n",
    "        classification = \"subjective\"\n",
    "    else:\n",
    "        classification = \"objective\"\n",
    "\n",
    "    subjectivity_probability = round(sigmoid_output.item(), 2)\n",
    "\n",
    "    return classification, subjectivity_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uALN6ulsyOY7"
   },
   "source": [
    "## Load classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-d1l0jomvTkd",
    "outputId": "a4780c7a-dda8-4af0-f29a-2db4fb2c8984"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=100)  # embedding size = 100\n",
    "\n",
    "# load the saved model for use with Gradio\n",
    "classifier_model = ClassifierModel(glove.vectors)\n",
    "classifier_model.load_state_dict(torch.load('classifier_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhzo9Ay9yT-_"
   },
   "source": [
    "## set up and run gradio application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaSwNTtnELRc"
   },
   "source": [
    "### \"gradio_classifier\" function\n",
    "\n",
    "DESCRIPTION:\n",
    "      this function is called when a a certain gradio button is pressed. it\n",
    "      returns the model classification of the input sentence along with the\n",
    "      probability of the sentence being subjective.\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "      input_sentence: the sentence written in the gradio text box to be classified for subjectivity/objectivity\n",
    "\n",
    "OUTPUTS:\n",
    "\n",
    "      model_classification: classification of our used model\n",
    "\n",
    "      model_prob: probability/confidence that model thinks the sentence is subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPvzIOysENdh"
   },
   "outputs": [],
   "source": [
    "def gradio_classifier(input_sentence):\n",
    "    tokenized_sentence = tokenize_sentence(input_sentence)\n",
    "    sentence_len = tokenized_sentence.shape[0]\n",
    "\n",
    "    model_classification, model_prob = classify_sentence(classifier_model, tokenized_sentence)\n",
    "\n",
    "    return model_classification, model_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdlpwF4OEsSM"
   },
   "source": [
    "### run the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "p05L3jNHyJhN",
    "outputId": "1cb0f5a2-ebfb-441f-d339-d4da8e923653"
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    input_sentence = gr.Textbox(label=\"Sentence\")\n",
    "    model_class_output = gr.Textbox(label=\"Model classification\")\n",
    "    model_prob = gr.Textbox(label=\"Model subjectivity probability\")\n",
    "\n",
    "    classify_button = gr.Button(\"Classify\")\n",
    "    classify_button.click(fn=gradio_classifier, inputs=input_sentence,\n",
    "                          outputs= [model_class_output, model_prob])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V5Ih-IpEzG3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
